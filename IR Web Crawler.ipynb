{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import queue\n",
    "import urllib.parse\n",
    "import time\n",
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urldefrag\n",
    "from reppy.cache import RobotsCache\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-5cfcbce9227c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobotsCache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdb_link\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mconn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'conn' is not defined"
     ]
    }
   ],
   "source": [
    "BACKQUEUES= 3\n",
    "THREADS= BACKQUEUES*3\n",
    "FRONTQUEUES= 5\n",
    "WAITTIME= 15 \n",
    "out_file_path=\"C:/Users/ahmed/Desktop/out_URLs.txt\"\n",
    "cache = RobotsCache(500)\n",
    "db_link=False\n",
    "crawled = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_file_handler():\n",
    "    if not os.path.isfile(out_file_path):\n",
    "            out_file = open(out_file_path,'w')\n",
    "            return out_file\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class frontier:\n",
    "    # add the code for frontier here\n",
    "    # should have functions __init__, get_URL, add_URLs, add_to_backqueue\n",
    "\n",
    "    ######### add_to_backQueue ###############################\n",
    "\n",
    "    def add_to_backqueue(self, pos):\n",
    "        counter = 0\n",
    "        checker = False\n",
    "        flag = False\n",
    "        while checker == False and counter < 3:\n",
    "            flag = False\n",
    "            FQ = random.randint(1, FRONTQUEUES)\n",
    "            FQ = FQ - 1\n",
    "            if not self.FrontQueues[FQ].empty():\n",
    "                URL = self.FrontQueues[FQ].get()\n",
    "                for i in range(len(self.BackQueues)):\n",
    "                    if i != pos and not self.BackQueues[i].empty():\n",
    "                        d = self.BackQueues[i].queue\n",
    "                        url_existing = urllib.parse.urlparse(d[0])\n",
    "                        url_adding = urllib.parse.urlparse(URL)\n",
    "                        if url_existing.netloc == url_adding.netloc:\n",
    "                            self.BackQueues[i].put(URL)\n",
    "                            flag = True\n",
    "                if flag == False:\n",
    "                    self.BackQueues[pos].put(URL)\n",
    "                    checker = True\n",
    "            counter = counter + 1\n",
    "\n",
    "        if checker == False:\n",
    "            for j in range(len(self.FrontQueues)):\n",
    "                if not self.FrontQueues[j].empty():\n",
    "                    URL = self.FrontQueues[j].get()\n",
    "                    for i in range(len(self.BackQueues)):\n",
    "                        if i != pos and not self.BackQueues[i].empty():\n",
    "                            d = self.BackQueues[i].queue\n",
    "                            url_existing = urllib.parse.urlparse(d[0])\n",
    "                            url_adding = urllib.parse.urlparse(URL)\n",
    "                            if url_existing.netloc == url_adding.netloc:\n",
    "                                self.BackQueues[i].put(URL)\n",
    "                                flag = True\n",
    "                    if flag == False:\n",
    "                        self.BackQueues[pos].put(URL)\n",
    "\n",
    "                        break\n",
    "\n",
    "    ############# _INIT_ #############\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initializer\n",
    "        '''\n",
    "        self.heap = []\n",
    "        self.FrontQueues = []\n",
    "        for i in range(FRONTQUEUES):\n",
    "            self.FrontQueues.append(queue.Queue())\n",
    "\n",
    "        self.BackQueues = []\n",
    "        for i in range(BACKQUEUES):\n",
    "            self.BackQueues.append(queue.Queue())\n",
    "\n",
    "        fpath = \"C:/Users/ahmed/Desktop/Seed URLs.txt\"\n",
    "        if not os.path.isfile(fpath):\n",
    "            print(\"Seed URLs File does not exist\")\n",
    "        else:\n",
    "            with open(fpath, \"r\") as file:\n",
    "                for line in file:\n",
    "                    url = line.strip()\n",
    "                    i = prioritizer(url, FRONTQUEUES)\n",
    "                    self.FrontQueues[i - 1].put(url)\n",
    "\n",
    "    ######## get_URL########################\n",
    "\n",
    "    def get_URL(self):\n",
    "        index_min = min(range(len(self.heap)), key=self.heap.__getitem__)\n",
    "        url = self.BackQueues[index_min].get()\n",
    "        crawled.append(url)\n",
    "        if self.BackQueues[index_min].empty():\n",
    "            self.add_to_backqueue(index_min)\n",
    "            d = self.BackQueues[index_min].queue\n",
    "            print(len(d))\n",
    "            url_existing = urllib.parse.urlparse(d[0])\n",
    "            url_adding = urllib.parse.urlparse(url)\n",
    "            if url_existing.netloc == url_adding.netloc:\n",
    "                self.heap[index_min] == self.heap[index_min] + 15\n",
    "        else:\n",
    "            self.heap[index_min] == self.heap[index_min] + 15\n",
    "        return url\n",
    "\n",
    "    ########### add_URLs ###################\n",
    "\n",
    "    def add_URLs(self, URLs):\n",
    "        if len(URLs) > 0:\n",
    "            for i in URLs:\n",
    "                self.FrontQueues[prioritizer(i, FRONTQUEUES)].put(i)\n",
    "\n",
    "    ######## prep_BackQueue#################\n",
    "    def prep_BackQueue(self):\n",
    "\n",
    "        for i in range(len(self.BackQueues)):\n",
    "            self.add_to_backqueue(i)\n",
    "            self.heap.insert(i, 0)\n",
    "\n",
    "\n",
    "def prioritizer(URL, f):\n",
    "    \"\"\"\n",
    "    Take URL and returns priority from 1 to F\n",
    "    Right now it like a stub function.\n",
    "    It will return a random number from 1 to f for given inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    return random.randint(1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pyodbc.connect('Driver={SQL Server};'\n",
    "                      'Server=DESKTOP-PRRQ2CQ;'\n",
    "                      'Database=Crawler;'\n",
    "                      'Trusted_Connection=yes;')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('CREATE TABLE Crawled (URL varchar(100), Data TEXT)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(url):\n",
    "    w=urllib.request.urlopen(url)\n",
    "    data=w.read()\n",
    "    data_str = data.decode('utf-8')\n",
    "    cursor.execute('''\n",
    "                INSERT INTO Crawled (URL, Data)\n",
    "                VALUES\n",
    "                ('%s',%s)\n",
    "                \n",
    "                '''%(url,data_str))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(data,url):\n",
    "    soup=BeautifulSoup(data,'html.parser')\n",
    "    urls=[]\n",
    "    for i in soup.find_all(\"a\"):\n",
    "        link = i.get(\"href\")\n",
    "        if isUrlAbsolute(link):\n",
    "            link=urljoin(url,link)\n",
    "        urls.append(link)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isUrlAbsolute(url):\n",
    "    return (\"https:\" not in url and \"http:\" not in url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def urlFilter(urls):\n",
    "    filtered = []\n",
    "    for i in urls:\n",
    "        if cache.allowed(i, 'foo'):\n",
    "            filtered.append(i)\n",
    "    return filtered\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyodbc.Cursor at 0x265492ea1b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://en.wikipedia.org/wiki/Wikipedia:General_disclaimer\n",
      "DefragResult(url='http://en.wikipedia.org/wiki/Wikipedia:General_disclaimer', fragment='')\n",
      "http://en.wikipedia.org/wiki/Wikipedia:General_disclaimer\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin, urldefrag\n",
    "base_url = 'http://en.wikipedia.org/wiki/Main_Page'\n",
    "relative_url = '/wiki/Wikipedia:General_disclaimer'\n",
    "url = urljoin(base_url, relative_url)\n",
    "print (url)\n",
    "ux = urldefrag(url)\n",
    "print(ux)\n",
    "print (ux[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc \n",
    "connection = pyodbc.connect('Driver={SQL Server};'\n",
    "                      'Server=DESKTOP-PRRQ2CQ;'\n",
    "                      'Database=test_db;'\n",
    "                      'Trusted_Connection=yes;')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = connection.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyodbc.Cursor at 0x265492ea130>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = \"www.google.com\"\n",
    "b = \"Minnn\"\n",
    "\n",
    "cur.execute('''\n",
    "                INSERT INTO table_name (URL, Data)\n",
    "                VALUES\n",
    "                ('%s','%s')\n",
    "                \n",
    "                '''%(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.robotparser\n",
    "rp = urllib.robotparser.RobotFileParser()\n",
    "rp.set_url(\"https://en.wikipedia.org/robots.txt\")\n",
    "rp.read()\n",
    "rrate = rp.request_rate(\"*\")\n",
    "\n",
    "rp.can_fetch(\"*\", \"/wiki/Wikipedia:Vandalensperrung/\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>robotstxt_url</th>\n",
       "      <th>user_agent</th>\n",
       "      <th>url_path</th>\n",
       "      <th>can_fetch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://en.wikipedia.org/robots.txt</td>\n",
       "      <td>*</td>\n",
       "      <td>/wiki/Wikipedia:Vandalensperrung/</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         robotstxt_url user_agent  \\\n",
       "0  https://en.wikipedia.org/robots.txt          *   \n",
       "\n",
       "                            url_path  can_fetch  \n",
       "0  /wiki/Wikipedia:Vandalensperrung/      False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from advertools import robotstxt_test\n",
    "robotstxt_test('https://en.wikipedia.org/wiki/Wikipedia:Vandalensperrung/', user_agents=['*'], urls=[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cache = RobotsCache(100)\n",
    "cache.allowed('https://en.wikipedia.org/wiki/Wikipedia:Vandalensperrung/', 'foo')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.allowed('https://en.wikipedia.org', 'foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
